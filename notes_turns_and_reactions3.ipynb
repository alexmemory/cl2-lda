{
 "metadata": {
  "name": "notes_turns_and_reactions3"
 }, 
 "nbformat": 2, 
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown", 
     "source": [
      "# Reactions per turn"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "Focusing on generating evaluation results for our specific tasks."
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "import pandas as pd", 
      "import reactions", 
      "import nltk", 
      "import random", 
      "import matplotlib.pyplot as plt", 
      "from pandas.tools.plotting import scatter_matrix", 
      "from nltk.corpus import stopwords"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 1
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "%time r = reactions.link_reactions_to_transcript('data/reactions_oct3_4project.csv','corpora/oct3_coded_transcript_sync.csv')"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "CPU times: user 8.41 s, sys: 0.52 s, total: 8.93 s", 
        "Wall time: 8.93 s"
       ]
      }
     ], 
     "prompt_number": 2
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "r2 = r.copy()", 
      "#del r2[\"Sync'd start\"]", 
      "#del r2[\"Sync'd end\"]", 
      "del r2[\"Time\"]", 
      "del r2[\"Speaker\"]", 
      "#r2.head(2)"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 3
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "## Political questionnaire data"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "%time p = reactions.split_reactions_file('data/reactions_oct3_4project.csv')['quest_political']"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "CPU times: user 4.79 s, sys: 0.37 s, total: 5.16 s", 
        "Wall time: 5.16 s"
       ]
      }
     ], 
     "prompt_number": 331
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "p2 = p[['UserID','party_1','political_views_2','candidate_choice_3','confidence_in_choice_4','likely_to_vote_5','candidate_preferred_29']]"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 11
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "Simplify party membership into R/D/oth"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "p2['party'] = p2.party_1.apply(lambda a: {'closest to democratic party':'democrat', ", 
      "                                          'lean democrat':'democrat',", 
      "                                          'lean republican':'republican',", 
      "                                          'closest to republican party':'republican'}.get(a,'other'))", 
      "p2['candidate'] = p2.candidate_choice_3", 
      "#p2['candidate'] = p2.candidate_choice_3.apply(lambda a: {'obama':'democrat', ", 
      "#                                          'lean democrat':'democrat',", 
      "#                                          'lean republican':'republican',", 
      "#                                          'closest to republican party':'republican'}.get(a,'other'))"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 333
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "### Merge political questionnaire with reactions"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "%time r3 = r2.merge(p2[['UserID','party','candidate']])", 
      "print 'pre-merge:',len(r2),'post-merge:',len(r3)", 
      "#r3.head(2)"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "CPU times: user 0.62 s, sys: 0.05 s, total: 0.66 s", 
        "Wall time: 0.66 s", 
        "pre-merge: 189015 post-merge: 189015"
       ]
      }
     ], 
     "prompt_number": 334
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "Limit to reactions to the speaker of the ***current turn***."
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "r4 = r3[r3.Reaction_who == r3.Speaker_name]", 
      "print 'before:',len(r3),'current-speaker-only:',len(r4), 'difference:',len(r4)-len(r3), 1.0*(len(r4)-len(r3))/len(r4),'percent'"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "before: 189015 current-speaker-only: 156622 difference: -32393 -0.206822796287 percent"
       ]
      }
     ], 
     "prompt_number": 335
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "## Group by turn"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "Statements"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [
      "st = r4.groupby(['statement']).first()[['Speaker_name','Transcript','turn',\"Sync'd start\",\"Sync'd end\"]]"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 17
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "Turns"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "t = pd.DataFrame({'speaker':st.groupby('turn').first().Speaker_name, ", 
      "                  'start':st.groupby('turn').first()[\"Sync'd start\"],", 
      "                  'end':st.groupby('turn').last()[\"Sync'd end\"],", 
      "                  #'reactions':r4.groupby('turn').count().Speaker_name, ", 
      "                  'reactions_oba':r4[(r4.candidate=='obama')].groupby('turn').count().turn,", 
      "                  'reactions_rom':r4[(r4.candidate=='romney')].groupby('turn').count().turn,", 
      "                  'spin_rom':r4[(r4.candidate=='romney') & (r4.Reaction_what=='Spin')].groupby('turn').count().turn,", 
      "                  'statements':st.groupby('turn').count().turn, ", 
      "                  'text':st.groupby('turn').apply(lambda x: ''.join(x.Transcript)),", 
      "                  'agree':r4[r4.Reaction_what=='Agree'].groupby('turn').count().turn,", 
      "                  #'agree_dem':r4[(r4.party=='democrat') & (r4.Reaction_what=='Agree')].groupby('turn').count().turn,", 
      "                  #'agree_rep':r4[(r4.party=='republican') & (r4.Reaction_what=='Agree')].groupby('turn').count().turn,", 
      "                  'agree_oba':r4[(r4.candidate=='obama') & (r4.Reaction_what=='Agree')].groupby('turn').count().turn,", 
      "                  'agree_rom':r4[(r4.candidate=='romney') & (r4.Reaction_what=='Agree')].groupby('turn').count().turn,", 
      "                  'disagree':r4[r4.Reaction_what=='Disagree'].groupby('turn').count().turn,", 
      "                  #'disagree_dem':r4[(r4.party=='democrat') & (r4.Reaction_what=='Disagree')].groupby('turn').count().turn,", 
      "                  #'disagree_rep':r4[(r4.party=='republican') & (r4.Reaction_what=='Disagree')].groupby('turn').count().turn,", 
      "                  'disagree_oba':r4[(r4.candidate=='obama') & (r4.Reaction_what=='Disagree')].groupby('turn').count().turn,", 
      "                  'disagree_rom':r4[(r4.candidate=='romney') & (r4.Reaction_what=='Disagree')].groupby('turn').count().turn,", 
      "                  'dodge_oba':r4[(r4.candidate=='obama') & (r4.Reaction_what=='Dodge')].groupby('turn').count().turn,", 
      "                  'dodge_rom':r4[(r4.candidate=='romney') & (r4.Reaction_what=='Dodge')].groupby('turn').count().turn,", 
      "                  'dodge':r4[r4.Reaction_what=='Dodge'].groupby('turn').count().turn,", 
      "                  'spin_oba':r4[(r4.candidate=='obama') & (r4.Reaction_what=='Spin')].groupby('turn').count().turn,", 
      "                  'spin_rom':r4[(r4.candidate=='romney') & (r4.Reaction_what=='Spin')].groupby('turn').count().turn,", 
      "                  'spin':r4[r4.Reaction_what=='Spin'].groupby('turn').count().turn,", 
      "                  })", 
      "tmpstart = pd.to_datetime(t.start)", 
      "tmpend = pd.to_datetime(t.end)", 
      "t['dur'] = (tmpend - tmpstart)", 
      "t.duration = 1.0 * t.dur / 1000000000.0", 
      "t['words'] = t.text.apply(lambda txt: [tok.lower() for tok in nltk.tokenize.word_tokenize(txt) if tok.isalpha()])", 
      "t['word_count'] = t.words.apply(lambda words: len(words))", 
      "#t['r_per_st'] = 1.0 * t.reactions / t.statements", 
      "#t['r_per_w'] = 1.0 * t.reactions / t.word_count", 
      "#t['r_per_sec'] = 1.0 * t.reactions / t.dur", 
      "t['rps_oba'] = 1.0 * t.reactions_oba / t.dur", 
      "t['rps_rom'] = 1.0 * t.reactions_rom / t.dur", 
      "#t['sd_per_sec'] = 1.0 * (t.spin + t.dodge) / t.dur", 
      "t['sdps_oba'] = 1.0 * (t.spin_oba + t.dodge_oba) / t.dur", 
      "t['sdps_rom'] = 1.0 * (t.spin_rom + t.dodge_rom) / t.dur", 
      "#t['a_to_d_dems'] = t.agree_dem / t.disagree_dem", 
      "#t['a_to_d_reps'] = t.agree_rep / t.disagree_rep", 
      "t['a_to_d_oba'] = t.agree_oba / t.disagree_oba", 
      "t['a_to_d_rom'] = t.agree_rom / t.disagree_rom", 
      "", 
      "del t['agree']", 
      "#del t['agree_dem']", 
      "#del t['agree_rep']", 
      "del t['disagree']", 
      "#del t['disagree_dem']", 
      "#del t['disagree_rep']", 
      "del t['dodge']", 
      "del t['spin']", 
      "#del t['r_per_st']", 
      "#del t['r_per_w']", 
      "del t['start']", 
      "del t['end']", 
      "del t['dur']"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 357
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "all_words = [w for word_list in t.words for w in word_list]", 
      "all_bigrams = nltk.bigrams(all_words)", 
      "ranked_unigrams = nltk.FreqDist(all_words).keys()", 
      "ranked_bigrams = nltk.FreqDist(all_bigrams).keys()", 
      "MAX_FEATURES = 200 # avoid overfitting", 
      "t['unigrams'] = t.words.apply(lambda words: {w:True for w in words if w in ranked_unigrams[:MAX_FEATURES] and not w in stopwords.words('english')})", 
      "t['bigrams'] = t.words.apply(lambda words: {\"%s_%s\"%(w1,w2):True for w1,w2 in nltk.bigrams(words) if ", 
      "                                            (w1,w2 in ranked_bigrams[:MAX_FEATURES]) and", 
      "                                            (not w1 in stopwords.words('english')) and", 
      "                                            (not w2 in stopwords.words('english'))", 
      "                                            })"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 379
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "### Filter"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "For now, we get rid of the really short turns."
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "#MIN_WORDS = 30 # good results", 
      "MIN_WORDS = 0 # this really affects the republican results strongly", 
      "t2 = t[t.word_count >= MIN_WORDS]", 
      "print len(t),'->',len(t2)"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "181 -> 181"
       ]
      }
     ], 
     "prompt_number": 360
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "## Crossvalidation code"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "def cv(df, num_folds = 10, classifier=nltk.NaiveBayesClassifier, maxent_params=None):", 
      "    df = df.copy()", 
      "    df = df.reindex(np.random.permutation(e2b.index))", 
      "    fold_size = len(df)/num_folds", 
      "    fold_starts = range(0, len(df)+fold_size, fold_size)", 
      "    folds = zip(fs,fs[1:])", 
      "    accs = []", 
      "    for (first,last) in folds:", 
      "        test_rows = df.index[first:last]", 
      "        tst = df.ix[test_rows]", 
      "        trn = df.drop(test_rows)", 
      "        if maxent_params == None:", 
      "            cl = classifier.train(zip(trn.features, trn.label))", 
      "        else:", 
      "            cl = classifier.train(zip(trn.features, trn.label), ", 
      "                                  algorithm=maxent_params['algorithm'], ", 
      "                                  max_iter=maxent_params['max_iter'],", 
      "                                  trace=maxent_params['trace'])", 
      "        accs.append(nltk.classify.accuracy(cl, zip(tst.unigrams, tst.label)))", 
      "    return {'mean':mean(accs),'stdev':std(accs)}"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 361
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "## Task 1: Reactions per second"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "#a = {'a1':1,'a2':2}", 
      "#b = {'b1':1,'b2':2}", 
      "#dict(a.items() + b.items())"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 362
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "t2['features'] = t2.unigrams", 
      "#t.map(lambda row: dict(row['unigrams'].items() + row['bigrams'].items()))"
     ], 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 363
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "e1 = t2.copy()", 
      "e1['label'] = e1.rps_oba >= e1.rps_oba.quantile(.5)", 
      "print 'DT', cv(e1, classifier=nltk.classify.DecisionTreeClassifier)", 
      "print 'ME', cv(e1, classifier=nltk.classify.MaxentClassifier, ", 
      "               maxent_params={'algorithm':nltk.classify.MaxentClassifier.ALGORITHMS[0], ", 
      "                              'max_iter':25,", 
      "                              'trace':0})", 
      "print 'NB', cv(e1, classifier=nltk.NaiveBayesClassifier)"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "DT "
       ]
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "{'stdev': 0.076930925816207196, 'mean': 0.84285714285714275}", 
        "ME "
       ]
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "{'stdev': 0.15971914124998499, 'mean': 0.78571428571428559}", 
        "NB "
       ]
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "{'stdev': 0.091473203391897809, 'mean': 0.75714285714285712}"
       ]
      }
     ], 
     "prompt_number": 366
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "e1 = t2.copy()", 
      "e1['label'] = e1.rps_rom >= e1.rps_rom.quantile(.5)", 
      "print 'DT', cv(e1, classifier=nltk.classify.DecisionTreeClassifier)", 
      "print 'ME', cv(e1, classifier=nltk.classify.MaxentClassifier, ", 
      "               maxent_params={'algorithm':nltk.classify.MaxentClassifier.ALGORITHMS[0], ", 
      "                              'max_iter':25,", 
      "                              'trace':0})", 
      "print 'NB', cv(e1, classifier=nltk.NaiveBayesClassifier)"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "DT "
       ]
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "{'stdev': 0.16659862556700861, 'mean': 0.82857142857142851}", 
        "ME "
       ]
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "{'stdev': 0.16225452416572211, 'mean': 0.84285714285714286}", 
        "NB "
       ]
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "{'stdev': 0.14568627181693672, 'mean': 0.77142857142857135}"
       ]
      }
     ], 
     "prompt_number": 367
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "## Task 2a: Majority agrees with speaker  THIS IS TASK 2"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "Democrats"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "e2ad = t2.copy()", 
      "#e2ad['label'] = e2ad.agree_dem >= e2ad.disagree_dem", 
      "e2ad['label'] = e2ad.agree_oba >= e2ad.disagree_oba", 
      "print 'DT', cv(e2ad, classifier=nltk.classify.DecisionTreeClassifier)", 
      "print 'ME', cv(e2ad, classifier=nltk.classify.MaxentClassifier, ", 
      "               maxent_params={'algorithm':nltk.classify.MaxentClassifier.ALGORITHMS[0], ", 
      "                              'max_iter':25,", 
      "                              'trace':0})", 
      "print 'NB', cv(e2ad, classifier=nltk.NaiveBayesClassifier)"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "DT "
       ]
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "{'stdev': 0.21946130708196024, 'mean': 0.74285714285714277}", 
        "ME "
       ]
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "{'stdev': 0.15714285714285714, 'mean': 0.75714285714285712}", 
        "NB "
       ]
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "{'stdev': 0.11866605518454393, 'mean': 0.87142857142857133}"
       ]
      }
     ], 
     "prompt_number": 368
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "Republicans"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "e2ar = t2.copy()", 
      "#e2ar['label'] = e2ar.agree_rep >= e2ar.disagree_rep", 
      "e2ar['label'] = e2ar.agree_rom >= e2ar.disagree_rom", 
      "print 'DT', cv(e2ar, classifier=nltk.classify.DecisionTreeClassifier)", 
      "print 'ME', cv(e2ar, classifier=nltk.classify.MaxentClassifier, ", 
      "               maxent_params={'algorithm':nltk.classify.MaxentClassifier.ALGORITHMS[0], ", 
      "                              'max_iter':25,", 
      "                              'trace':0})", 
      "print 'NB', cv(e2ar, classifier=nltk.NaiveBayesClassifier)"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "DT "
       ]
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "{'stdev': 0.12453996981544781, 'mean': 0.82857142857142851}", 
        "ME "
       ]
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "{'stdev': 0.069985421222376484, 'mean': 0.79999999999999993}", 
        "NB "
       ]
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "{'stdev': 0.11517511068997928, 'mean': 0.49999999999999989}"
       ]
      }
     ], 
     "prompt_number": 369
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "## Task 2b: Ratio agree-to-disagree above median"
     ]
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "Democrats"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "e2bd = t2.copy()", 
      "#e2bd['label'] = e2bd.a_to_d_dems >= e2bd.a_to_d_dems.quantile(.5)", 
      "e2bd['label'] = e2bd.a_to_d_oba >= e2bd.a_to_d_oba.quantile(.5)", 
      "print 'DT', cv(e2bd, classifier=nltk.classify.DecisionTreeClassifier)", 
      "print 'ME', cv(e2bd, classifier=nltk.classify.MaxentClassifier, ", 
      "               maxent_params={'algorithm':nltk.classify.MaxentClassifier.ALGORITHMS[0], ", 
      "                              'max_iter':25,", 
      "                              'trace':0})", 
      "print 'NB', cv(e2bd, classifier=nltk.NaiveBayesClassifier)"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "DT "
       ]
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "{'stdev': 0.11428571428571427, 'mean': 0.77142857142857146}", 
        "ME "
       ]
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "{'stdev': 0.10000000000000001, 'mean': 0.84285714285714275}", 
        "NB "
       ]
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "{'stdev': 0.11065666703449761, 'mean': 0.8571428571428571}"
       ]
      }
     ], 
     "prompt_number": 370
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "Republicans"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "e2br = t2.copy()", 
      "#e2br['label'] = e2br.a_to_d_reps >= e2br.a_to_d_reps.quantile(.5)", 
      "e2br['label'] = e2br.a_to_d_rom >= e2br.a_to_d_rom.quantile(.5)", 
      "print 'DT', cv(e2br, classifier=nltk.classify.DecisionTreeClassifier)", 
      "print 'ME', cv(e2br, classifier=nltk.classify.MaxentClassifier, ", 
      "               maxent_params={'algorithm':nltk.classify.MaxentClassifier.ALGORITHMS[0], ", 
      "                              'max_iter':25,", 
      "                              'trace':0})", 
      "print 'NB', cv(e2br, classifier=nltk.NaiveBayesClassifier)"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "DT "
       ]
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "{'stdev': 0.16963345838625596, 'mean': 0.75714285714285701}", 
        "ME "
       ]
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "{'stdev': 0.20652617564001374, 'mean': 0.69999999999999996}", 
        "NB "
       ]
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "{'stdev': 0.20253495541082608, 'mean': 0.61428571428571421}"
       ]
      }
     ], 
     "prompt_number": 371
    }, 
    {
     "cell_type": "markdown", 
     "source": [
      "## Task 3: Spins+dodges per second above median"
     ]
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "e3 = t2.copy()", 
      "e3['label'] = e3.sdps_oba >= e3.sdps_oba.quantile(.5)", 
      "print 'DT', cv(e3, classifier=nltk.classify.DecisionTreeClassifier)", 
      "print 'ME', cv(e3, classifier=nltk.classify.MaxentClassifier, ", 
      "               maxent_params={'algorithm':nltk.classify.MaxentClassifier.ALGORITHMS[0], ", 
      "                              'max_iter':25,", 
      "                              'trace':0})", 
      "print 'NB', cv(e3, classifier=nltk.NaiveBayesClassifier)"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "DT "
       ]
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "{'stdev': 0.13997084244475305, 'mean': 0.82857142857142851}", 
        "ME "
       ]
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "{'stdev': 0.090350790290525118, 'mean': 0.85714285714285698}", 
        "NB "
       ]
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "{'stdev': 0.11157499537009508, 'mean': 0.81428571428571406}"
       ]
      }
     ], 
     "prompt_number": 372
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "e3 = t2.copy()", 
      "e3['label'] = e3.sdps_rom >= e3.sdps_rom.quantile(.5)", 
      "print 'DT', cv(e3, classifier=nltk.classify.DecisionTreeClassifier)", 
      "print 'ME', cv(e3, classifier=nltk.classify.MaxentClassifier, ", 
      "               maxent_params={'algorithm':nltk.classify.MaxentClassifier.ALGORITHMS[0], ", 
      "                              'max_iter':25,", 
      "                              'trace':0})", 
      "print 'NB', cv(e3, classifier=nltk.NaiveBayesClassifier)"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "DT "
       ]
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "{'stdev': 0.14568627181693672, 'mean': 0.79999999999999993}", 
        "ME "
       ]
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "{'stdev': 0.13477115902938006, 'mean': 0.84285714285714286}", 
        "NB "
       ]
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "{'stdev': 0.23211538298959888, 'mean': 0.48571428571428565}"
       ]
      }
     ], 
     "prompt_number": 373
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [], 
     "language": "python", 
     "outputs": []
    }
   ]
  }
 ]
}