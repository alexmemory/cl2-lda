\documentclass[12pt]{article}
\usepackage{lingmacros}
\usepackage{tree-dvips}
\usepackage{graphicx}
\usepackage{float}

\begin{document}

\subsection{Predicting Reactions with Labeled Topics}

The Decision Tree classifier performed by far the best on Task 1, with $69.5\%$  accuracy on reactions by Obama supporters and $71.9\%$ accuracy on reactions by Romney supporters. The MaxEnt and Naive Bayes classifiers performed poorly. On Task 2, all classifiers performed surprisingly poorly, with the best accuracy being $58.9\%$ using a Naive Bayes classifier. The best performance was on Task 3, on which all three classifiers scored in the low $80\%$ range. The MaxEnt and Naive Bayes classifiers scored the highest with $82.3\%$ and $81.3\%$ accuracy on reactions by Obama and Romney supporters, respectively.


The highest-information gain features provide an interesting point of analysis. Task 1 involves predicting the overall volume of reactions based on the mixture of topics. The most useful features for this prediction tell us what topics users tended to respond to. For Obama supporters, these are "labor/employment/immigration," "education," and "health." For Romney supporters, they are "government operations," "macroeconomics," and "education."

Task 1 is somewhat overgeneral because the total number of reactions does not include information about what the reactions are. Task 3 involves predicting specific types of reactions, "spin" and "dodge," which both involve deception. In Table ~\ref{tab:task3boydstun}, "candidate personal information" is a good predictor for both candidates. A reasonable conclusion of these results is that when a candidate tells a personal anecdote, users from both parties tend to react as if it is an attempt to "spin" or "dodge" the question at hand.

For example, in Obama's response to a question about Social Security, the bold font text is hand-labeled as "candidate personal information:"

\footnotesize
\vspace*{.2in}
"...I want to talk about the values behind Social Security and Medicare and then talk about Medicare because that's the big driver of our deficits right now. \textbf{You know, my grandmother, some of you know, helped to raise me. My grandparents did. My grandfather died awhile back. My grandmother died three days before I was elected president. And she was fiercely independent.} ..."
\vspace*{.2in}
\normalsize

For Task 2, since the performance was so poor, reading into the meaning of the features with the highest information gain is probably unwise.

\subsection{Predicting Individual User Reactions}

As expected, using topic-based features in addition to user features improves classification accuracy. The maximum entropy classifier shows the greatest improvement, increasing from $45.2\%$ accuracy to $51.7\%$ accuracy.

This is an interesting result, since the maximum entropy classifier is the one classifier that makes no independence assumptions between the features. The increase in accuracy suggests that it does a better job of integrating the user features with the topic features. For instance, the maximum entropy classifier might be more capable of classifying the reaction of a user who indicates a certain topic is more important to him during a turn in which that topic is present.

From the confusion matrices, one can see that the majority of predictions fall into labels 0 and 2, corresponding to "Obama:Agree" and "Romney:Agree." In Figure \ref{fig:useronlyconfusion}, labels for "Romney:Disagree" are often confused with labels for "Obama:Agree." This makes sense, since a user who disagrees with Romney most likely agrees with Obama. Including topic features results in more predictions of labels 3 and 1, "Obama:Disagree" and "Romney:Disagree" (Figure \ref{fig:boydstunconfusion}).


\end{document}