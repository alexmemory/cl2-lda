%!TEX root =  cl2-lda.tex

\subsection{Results}

In contrast to our expectations, the unigram feature baseline tends to outperform both hand-labeled and LDA topic features. From the experiments we perform, it seems that individual words in the text are better predictors of reaction than topics inferred from the text.

However, using topics for classification does provide some interesting analysis on how users respond to certain topics. In Section 6.2, we learn which hand-labeled topics tend to generate more responses, and see that when a candidate resorts to personal anecdote the audience often interprets it as a "spin" or "dodge."

With the exception of Naive Bayes classifiers, it seems to be the case that simple LDA topics work well enough for predicting reactions to use them instead of coded topics. 
Unfortunately, it seems like the good performance is difficult to interpret and possibly very fragile.
Using vanilla LDA to obtain features for classification is probably not the best choice.

Finally, we see that using topic-based features boosts the performance of predicting individual users' reactions when combined with demographic features.

\subsection{Future Directions}

One direction for future work is to try out more complex topic models.
It would be interesting to see if you get better topics and/or predictions if they are learned in conjunction with a model like sLDA.
It would also be interesting to be agnostic about the number of topics with a nonparametric model like the hierarchical dirichlet process (HDP).
Finally, it would be very interesting to see if topic shift indicators inferred by the SITS model would improve dodge reaction predicitons.

Another direction would be to predict reactions for more specific problems.
This is doable with the infrastructure provided by format.py, database.py, svmlitegen.py in our source directory.
These files allow the user to specify any combination of user attributes in order to narrow down the reactions to track.

