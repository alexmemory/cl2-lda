
[Boydstun et. al, 2013] labels each phrase in the October 4th, 2012 debate corpus with a topic ID. Each ID corresponds to a significant topic in the debate, such as "Defense" or "Labor, Employment, and Immigration." Unlike the topics found by LDA, these are hand-selected, with a single topic per phrase instead of a distribution over topics.

We are interested in whether using these hand-labeled topics, as opposed to the topics discovered using LDA, will improve or worsen performance predicting users' reactions to the debate.

Since we are working at the turn level, not the phrase level, we assimilate all topic IDs within a turn into a distribution over topics for that turn.

As before, each labeled topic present in the corpus is a feature. For a given turn, the value of that feature is the fraction of phrases within the turn labeled with that topic.

Once again we evaluate classification accuracy on three tasks, or labels: Whether a turn generates more or less than the median number of reactions per second, whether a turn generates more 'agree' or 'disagree' reactions, and whether a turn generates more or less than the median number of spin or dodge reactions per second.

Classification is done with Mallet, using Decision Tree, Maximum Entropy, and Naive Bayes classifiers. We perform 10-fold cross-validation using the 197 turns to calculate test accuracy and standard deviation:

\begin{table}[H]
\begin{centering}
\begin{tabular}{ l | l | l }
Classifier & Obama voters & Romney voters \\
\hline
DecTree & \textbf{0.695} (.131) &  \textbf{.719} (.150) \\
MaxEnt & \textbf{0.498} (.146) &  \textbf{.386} (.129) \\
Naive & \textbf{0.454} (.142) &  \textbf{.386} (.130) \\
\end{tabular}
\caption{Task 1 Accuracy and StdDev}
\end{centering}
\end{table}

\begin{table}[H]
\begin{centering}
\begin{tabular}{ l | l | l }
Classifier & Obama voters & Romney voters \\
\hline
DecTree & \textbf{0.519} (.095) &  \textbf{.455} (.090) \\
MaxEnt & \textbf{0.577} (.092) &  \textbf{.562} (.141) \\
Naive & \textbf{0.589} (.093) &  \textbf{.567} (.144) \\
\end{tabular}
\caption{Task 2 Accuracy and StdDev}
\end{centering}
\end{table}

\begin{table}[H]
\begin{centering}
\begin{tabular}{ l | l | l }
Classifier & Obama voters & Romney voters \\
\hline
DecTree & \textbf{0.803} (.106) &  \textbf{.805} (.150) \\
MaxEnt & \textbf{0.823} (.122) &  \textbf{.813} (.154) \\
Naive & \textbf{0.823} (.122) &  \textbf{.813} (.154) \\
\end{tabular}
\caption{Task 3 Accuracy and StdDev}
\label{tab:task3boydstun}
\end{centering}
\end{table}

For each classification task, we also keep track of the features that the Mallet Decision Tree learner selects most frequently for having the highest information gain. We can then look up the corresponding topics for the most valuable features.

\begin{table*}[H]
\begin{centering}
\begin{tabular}{| l | l | l |}
\hline
  & Best feature (Obama) & Best Feature (Romney) \\
\hline
Task 1 & labor/employment/immigration & government operations \\
	    & education & macroeconomics \\
	    & health & education \\
	    \hline
Task 2 & labor/employment/immigration & labor/employment/immigration \\
	    & government operations & education \\
	    & social welfare & macroeconomics  \\
	    \hline
Task 3 & candidate personal information & health \\
	    & social welfare & government operations \\
	    & labor/employment/immigration & candidate personal information \\
	    \hline
\end{tabular}
\caption{3 Features with highest information gain}
\end{centering}
\end{table*}

The Decision Tree classifier performs by far the best on Task 1, with $69.5\%$  accuracy on reactions by Obama supporters and $71.9\%$ accuracy on reactions by Romney supporters. The MaxEnt and Naive Bayes classifiers perform poorly. On Task 2, all classifiers perform surprisingly poorly, with the best accuracy being $58.9\%$ using a Naive Bayes classifier. The best performance is on Task 3, on which all three classifiers score in the low $80\%$ range. The MaxEnt and Naive Bayes classifiers score the highest with $82.3\%$ and $81.3\%$ accuracy on reactions by Obama and Romney supporters, respectively.

The highest-information gain features provide an interesting point of analysis. Task 1 involves predicting the overall volume of reactions based on the mixture of topics. The most useful features for this prediction tell us what topics users tended to respond to. For Obama supporters, these are "labor/employment/immigration," "education," and "health." For Romney supporters, they are "government operations," "macroeconomics," and "education."

Task 1 is somewhat overgeneral because the total number of reactions does not include information about what the reactions are. Task 3 involves predicting specific types of reactions, "spin" and "dodge," which both involve deception. In Table ~\ref{tab:task3boydstun}, "candidate personal information" is a good predictor for both candidates. A reasonable conclusion of these results is that when a candidate tells a personal anecdote, users from both parties tend to react as if it is an attempt to "spin" or "dodge" the question at hand.

For example, in Obama's response to a question about Social Security, the bold font text is hand-labeled as "candidate personal information:"

\footnotesize
\vspace*{.2in}
"...I want to talk about the values behind Social Security and Medicare and then talk about Medicare because that's the big driver of our deficits right now. \textbf{You know, my grandmother, some of you know, helped to raise me. My grandparents did. My grandfather died awhile back. My grandmother died three days before I was elected president. And she was fiercely independent.} ..."
\vspace*{.2in}
\normalsize

For Task 2, since the performance was so poor, reading into the meaning of the features with the highest information gain is probably unwise.

