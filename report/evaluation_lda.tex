%!TEX root =  cl2-lda.tex

Latent Dirichlet Allocation (LDA) is a hierarchical Bayesian model.
In LDA, documents are modeled as random mixtures over latent topics and topics are distributions over words.

We ran Mallet's implementation of LDA on our training data (the October 3rd presidential debate and the presidential debate corpus) with hyperparameter optimization on.
The data was split into debate turns and stopwords were removed.
Figure \ref{fig:ldatopics} shows the top terms for each topic distribution generated by one run of LDA.
We chose to model 19 topics in order to agree with the number of topics in Boydstun's annotations.
The features for each document in our predictive models were the proportions of topics LDA allocated to those documents.

Since the focus of our project was classification, we did not put heavy focus on evaluating the LDA topics quantitatively.
Qualitatively, they are mediocre.
As shown in Figure \ref{fig:ldatopics}, roughly 12 out of 19 topics are coherent.
Only five of the remaining topics are marginally coherent (if you squint).
The remaining two are essentially incoherent.
This makes the results of classification using these topics as features all the more surprising.

Tables \ref{tab:task1lda} through \ref{tab:task3lda} show the average accuracy and standard deviation of each classifier on the test data.
The data was partitioned into training and test data with a 90:10 split and 10-fold cross validation.

Surprisingly, using LDA topic proportions actually performed \textit{better} for Task 1 and Task 2 with the Decision Tree and Maximum Entropy classifiers (Tables \ref{tab:task1lda} and \ref{tab:task2lda}) than the coded topic features (.
Although it was more than adequate for task 1 and task 2, it fell short of the coded topic classifiers for task three (Table \ref{tab:task3lda}).

The results with Naive Bayes are perhaps a little more revealing.
LDA features performed abyssmally in the Naive Bayes classifier.
Given the fact that the other classifiers performed so well, there must be an explanation.
Decision trees split the dataset on an attribute that gives them the maximum information gain (in this case, this was topic 0, which I claimed was incoherent) so it has a mechanism using features in order of importance.
Additionally, maximum entropy classifiers try to be as agnostic as possible about any attributes that aren't particularly helpful for classification.
Naive Bayes, however will attempt to use the probability of each feature given a class label to compute the posterior probability of that class.
It seems as though a select few of the LDA topics are very useful for classification and the rest detract substantially from the overall accuracy.

Interestingly, the LDA features performed better than unigram features for Task 1 in both Decision Trees and Maximum Entropy models.
Our hypothesis is that the features generated by LDA are inherently better for decision trees because of the lower dimensionality of the space.
Each time the tree is split, there are fewer training examples at for each branch.
It seems plausible that LDA is merging the unigram features into fewer more meaningful splits.

\begin{figure*}
\begin{flushleft}
\tablefirsthead{}
\tablehead{}
\tabletail{}
\tablelasttail{}
\begin{supertabular}{|m{4.8087597in}|m{1.1906599in}|}
\hline
{\selectlanguage{english} Top Terms} &
{\selectlanguage{english} Topic}\\\hline
{\selectlanguage{english} president america country states united didn made today respect \ difference american don fact
mistake make policy decisions question kids decision} &
{\selectlanguage{english} Incoherent}\\\hline
{\selectlanguage{english} campaign people country foreign american fact issues town lobbyists house congress tough white
honor running campaigns interested pledge john character} &
{\selectlanguage{english} Campaigning (?)}\\\hline
{\selectlanguage{english} \textbf{health care plan insurance medicare costs }cost government give companies
\textbf{system} buy seniors choice program lower billion \textbf{private} provide \textbf{premiums}} &
{\selectlanguage{english} Healthcare}\\\hline
{\selectlanguage{english} \textbf{drugs} don act \textbf{law} congress rights bill \textbf{crime} line \textbf{drug}
\textbf{police} \textbf{protect} patriot border \textbf{enforcement} legislation citizens fact enterprise
\textbf{fighting}} &
{\selectlanguage{english} Law Enforcement}\\\hline
{\selectlanguage{english} ve people don make ll time back work things years put lot good country america important
american president point thing} &
{\selectlanguage{english} Incoherent}\\\hline
{\selectlanguage{english} \textbf{iraq} \textbf{afghanistan} senator \textbf{troops} \textbf{strategy} obama
\textbf{pakistan} \textbf{russia} \textbf{war} \textbf{georgia} \textbf{qaeda} \textbf{al} mccain \textbf{military}
situation states understand russians \textbf{defeat} united} &
{\selectlanguage{english} Foreign Conflict}\\\hline
{\selectlanguage{english} \textbf{jobs} \textbf{trade} free agreement \textbf{job} country fair \textbf{industries}
\textbf{workers} \textbf{overseas} \textbf{business} standards base defense agreements wage century \textbf{pay}
minimum growing} &
{\selectlanguage{english} Jobs}\\\hline
{\selectlanguage{english} \textbf{governor} \textbf{romney} government \textbf{federal} \textbf{states} board approach
\textbf{obamacare} repeal conditions fact reason replace difference \textbf{cost} military investments crisis
opportunity \textbf{Massachusetts}} &
{\selectlanguage{english} Health Insurance Reform (?)}\\\hline
{\selectlanguage{english} \textbf{world} \textbf{war} \textbf{military} \textbf{troops} \textbf{national}
\textbf{security} \textbf{forces} \textbf{army} peace russia europe \textbf{nuclear} \textbf{draft} general question
\textbf{defense} \textbf{cold} union superpower democracy} &
{\selectlanguage{english} Defense}\\\hline
{\selectlanguage{english} \textbf{nuclear} \textbf{iran} \textbf{north} \textbf{korea} president \textbf{weapons}
\textbf{talks} threat senator united mccain \textbf{proliferation} sanctions countries israel involved china table
\ \ \ \ states ambassador} &
{\selectlanguage{english} Nuclear Weapons}\\\hline
{\selectlanguage{english} \textbf{education} \textbf{school} \textbf{schools} \textbf{child} money \textbf{children}
\textbf{kids} america system job \textbf{teachers} state \textbf{public} abortion program funding college continue
choice \ \ aids} &
{\selectlanguage{english} Education}\\\hline
{\selectlanguage{english} mr president \textbf{question} \textbf{senator} \textbf{minute} perot bush governor kerry
\textbf{minutes} clinton seconds \textbf{answer} \textbf{audience} \textbf{tonight} \textbf{debate} \textbf{questions}
presidential \ candidates sir} &
{\selectlanguage{english} Debate Phrases}\\\hline
{\selectlanguage{english} senator obama voted spending senate states united record party opposed constitution fought
reform bill marriage friends times issue justice \ completely} &
{\selectlanguage{english} Legislative Branch (?)}\\\hline
{\selectlanguage{english} \textbf{iraq} \textbf{war} world \textbf{saddam} \textbf{hussein} plan troops \textbf{weapons}
free opponent \ \ \ \ \textbf{bin} \textbf{osama} \textbf{laden} \textbf{terror} win wrong safe threat strong
intelligence} &
{\selectlanguage{english} Middle East / Terrorism}\\\hline
{\selectlanguage{english} \textbf{energy} \textbf{oil} \textbf{nuclear} \textbf{fuel} percent \textbf{technology}
\textbf{power} reduce stem united companies \textbf{coal} \textbf{drilling} \textbf{clean} \textbf{wind} states
\textbf{solar} \textbf{science} mccain issue} &
{\selectlanguage{english} Energy}\\\hline
{\selectlanguage{english} congress people economic years mr government american governor economy clinton control growth
spend social program change country interest programs jobs} &
{\selectlanguage{english} Clinton Years Economy (?)}\\\hline
{\selectlanguage{english} \textbf{tax} \textbf{taxes} \textbf{cut} \textbf{jobs} billion spending people \textbf{middle}
small \textbf{percent} \ \ \ \ plan pay \textbf{class} america \textbf{raise} budget \textbf{business} money
\textbf{income} deficit} &
{\selectlanguage{english} Taxes}\\\hline
{\selectlanguage{english} mccain sen senator obama crisis \textbf{economy} \textbf{street} \textbf{financial} question
\textbf{banks} \textbf{wall} \textbf{policy} regulation \textbf{economic} homes americans tonight system package fix} &
{\selectlanguage{english} Economy (?)}\\\hline
{\selectlanguage{english} \textbf{children} \textbf{people} \textbf{women} \textbf{family} person \textbf{love} american
\textbf{faith} \textbf{dream} \textbf{life} \textbf{values} great \textbf{woman} strong part \textbf{laughter}
\textbf{god} \textbf{personal} \textbf{wife} \textbf{daughters}} &
{\selectlanguage{english} Pathos}\\\hline
\end{supertabular}
\end{flushleft}
\caption{LDA topics}
\label{fig:ldatopics}
\end{figure*}

\begin{table}[H]
\begin{centering}
\begin{tabular}{ l | l | l }
Classifier & Obama voters & Romney voters \\
\hline
DecTree & \textbf{0.71} (0.05) &  \textbf{0.71} (0.07) \\
MaxEnt & \textbf{0.70} (.06) &  \textbf{0.68} (.05) \\
Naive & \textbf{0.20} (.05) &  \textbf{0.20} (.04) \\
\end{tabular}
\caption{Task 1 (lda features): Accuracy and StdDev}
\label{tab:task1lda}
\end{centering}
\end{table}

\begin{table}[H]
\begin{centering}
\begin{tabular}{ l | l | l }
Classifier & Obama Voters & Romney Voters \\
\hline
DecTree & \textbf{0.76} (0.08) &  \textbf{0.70} (.06) \\
MaxEnt & \textbf{0.74} (0.05) &  \textbf{0.71} (0.09) \\
Naive & \textbf{0.23} (0.04) &  \textbf{0.21} (0.05) \\
\end{tabular}
\caption{Task 2 (lda features): Accuracy and StdDev}
\label{tab:task2lda}
\end{centering}
\end{table}

\begin{table}[H]
\begin{centering}
\begin{tabular}{ l | l | l }
Classifier & Obama Voters & Romney Voters \\
\hline
DecTree & \textbf{0.74} (0.07) &  \textbf{0.77} (0.05) \\
MaxEnt & \textbf{0.70} (0.10) &  \textbf{0.71} (0.07) \\
Naive & \textbf{0.22} (0.05) &  \textbf{0.26} (0.04) \\
\end{tabular}
\caption{Task 3 (lda features): Accuracy and StdDev}
\label{tab:task3lda}
\end{centering}
\end{table}


